{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd0d3d3",
   "metadata": {},
   "source": [
    "# Atividade 1 - Ciência dos Dados\n",
    "1 - Efetuar a extração de 500 cursos de pós-graduação do link https://sucupira.capes.gov.br/programas/detalhamento/2099, dica, analise o site para se obter todo o conteúdo. De cada curso você deve extrair: código, areaBasica, areaAvaliacao, situação, cidade, mestrado, nota, situacaoMestrado, doutorado, codigoMestrado, situacaoDoutorado, notaMestrado, codigoDoutorado, cep, início e universidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df93834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "\n",
    "curso = []\n",
    "n = 1\n",
    "while len(curso) < 50: # mudar para extrair mais informações\n",
    "    url = \"https://sucupira.capes.gov.br/programas/detalhamento/{}\".format(1500 + n)\n",
    "    print(url)\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    if re.search(r'(?=.*Mestrado)(?=.*Doutorado)', soup.find('div', class_='row bg-cinza0 my-2').text):\n",
    "        div_q_item_label = soup.find_all('div', class_='q-item__label')\n",
    "\n",
    "        for i, text in enumerate(div_q_item_label[:-8]):\n",
    "            if i == 1:\n",
    "                resultado = re.search(r'\\((.*?)\\).*Nota\\s+(\\d+)', text.text)\n",
    "                if resultado:\n",
    "                    codigo = resultado.group(1)  \n",
    "                    nota = resultado.group(2)    \n",
    "            if i == 9:\n",
    "                area_basica = text.text\n",
    "            if i == 7:\n",
    "                area_avaliacao = text.text\n",
    "            if i == 15:\n",
    "                situacao = text.text\n",
    "\n",
    "        div_lista = soup.find_all('div', class_='item-lista')\n",
    "        for i, item in enumerate(div_lista):\n",
    "            span_tag = item.find('span')\n",
    "            if span_tag:\n",
    "                if i == 0:\n",
    "                    universidade = span_tag.get_text()\n",
    "\n",
    "        h6 = soup.find_all('h6')\n",
    "        for item in h6:\n",
    "            texto = item.get_text(strip=True)\n",
    "            if re.search(r'.*\\sMestrado', texto):\n",
    "                mestrado = texto\n",
    "            if re.search(r'.*\\sDoutorado', texto):\n",
    "                doutorado = texto  \n",
    "\n",
    "        info_mestr = soup.find(string=mestrado).find_next()\n",
    "        for item in info_mestr:\n",
    "            nota_mestrado = item.find('div', class_='q-badge').text\n",
    "            cod_mestrado = item.find_all('span')[0].strong.text\n",
    "            inicio_mestrado = item.find_all('span')[2].strong.text\n",
    "            sit_mestrado = item.find_all('span')[3].text\n",
    "\n",
    "        info_dout = soup.find(string=doutorado).find_next()\n",
    "        for item in info_dout:\n",
    "            nota_doutorado = item.find('div', class_='q-badge').text\n",
    "            cod_doutorado = item.find_all('span')[0].strong.text\n",
    "            inicio_doutorado = item.find_all('span')[2].strong.text\n",
    "            sit_doutorado = item.find_all('span')[3].text\n",
    "\n",
    "        curso.append([codigo, nota, area_basica, area_avaliacao, situacao, universidade, mestrado, nota_mestrado, cod_mestrado, inicio_mestrado, sit_mestrado, doutorado, nota_doutorado, cod_doutorado, inicio_doutorado, sit_doutorado])\n",
    "    \n",
    "    n+=1\n",
    "    driver.quit()\n",
    "\n",
    "with open('pos_graduacao.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['codigo', 'nota', 'area_basica', 'area_avaliacao', 'situacao', 'universidade', 'mestrado', 'nota_mestrado', 'cod_mestrado', 'inicio_mestrado', 'sit_mestrado', 'doutorado', 'nota_doutorado', 'cod_doutorado', 'inicio_doutorado', 'sit_doutorado'])\n",
    "    writer.writerows(curso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01761f9f",
   "metadata": {},
   "source": [
    "2- Efetuar a extração de duas urls diferentes através da linguagem Python, e ter como saída um documento dois csv contendo todos os dados extraídos. Submeter o código em Python\n",
    "\n",
    "**Url 1**\n",
    "\n",
    "https://books.toscrape.com/\n",
    "\n",
    "Campos extraídos:\n",
    "   - nome\n",
    "   - preço\n",
    "   - rating\n",
    "   - disponibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f72aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "import csv\n",
    "\n",
    "produtos = []\n",
    "n = 1\n",
    "while n <= 5:\n",
    "    url = 'https://books.toscrape.com/catalogue/page-{}.html'.format(n) \n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Edge(options = options)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    product_pod = soup.find_all('article', class_='product_pod')\n",
    "    for item in product_pod:\n",
    "        nome = item.img['alt']\n",
    "        preco = item.find('p', class_='price_color').text\n",
    "        disp = item.find('p', class_='instock availability').text.strip()\n",
    "        rating = item.find('p', class_='star-rating')['class'][1]\n",
    "        livro = [nome, preco, rating, disp]\n",
    "        produtos.append(livro)\n",
    "    n+=1\n",
    "    driver.quit()\n",
    "\n",
    "with open('produtos.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['nome', 'preço', 'rating', 'disponibilidade'])\n",
    "    writer.writerows(produtos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf078ee",
   "metadata": {},
   "source": [
    "**Url 2**\n",
    "\n",
    "https://quotes.toscrape.com/\n",
    "\n",
    "Campos extraídos:\n",
    "\n",
    "  - quote\n",
    "  - autor\n",
    "  - tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "import csv\n",
    "\n",
    "conteudos = []\n",
    "n = 1\n",
    "while n <= 5:\n",
    "    url = 'https://quotes.toscrape.com/page/{}'.format(n)\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Edge(options = options)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "    for item in quotes:\n",
    "        quote = item.find('span', class_='text').text\n",
    "        author = item.find('small', class_='author').text\n",
    "        tags = item.find('meta', class_='keywords')['content']\n",
    "        conteudo = [quote, author, tags]\n",
    "        conteudos.append(conteudo)\n",
    "    n+=1\n",
    "    driver.quit()\n",
    "\n",
    "with open('quotes.csv', mode='w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['quote', 'author', 'tags'])\n",
    "    writer.writerows(conteudos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c1ef4",
   "metadata": {},
   "source": [
    "3 - Os blogs e outros sites atualizados regularmente em geral têm uma página inicial com a postagem mais recente, além de um botão Previous (Anterior) na página que conduzirá você à postagem anterior. Então essa postagem também terá um botão Previous e assim sucessivamente, criando um percurso que conduz da página mais recente até a primeira postagem do site. Se quiser uma cópia do conteúdo do site para ler quando não estiver online, você poderá navegar manualmente por todas as páginas e salvar cada uma delas. Contudo esse é um trabalho bem maçante, portanto vamos criar um programa que faça isso. O XKCD é um webcomic popular para geeks com um site que se enquadra nessa estrutura. A página inicial em http://xkcd.com/ contém um botão Prev (Anterior) que conduz o usuário às tirinhas anteriores. Fazer download de cada tirinha manualmente demoraria muito tempo, porque não utilizarmos o conteúdo aprendido nesta aula? Seu programa deve fazer:\n",
    "\n",
    "- Carregar a página inicial de XKCD.\n",
    "- Salvar a imagem da tirinha que está nessa página.\n",
    "- Seguir o link para Previous Comic (Tirinha anterior).\n",
    "- Repetir até alcançar a primeira tirinha.\n",
    "\n",
    "Isso significa que seu código deverá fazer o seguinte:\n",
    "\n",
    "- Fazer download de páginas com o módulo requests.\n",
    "- Encontrar o URL da imagem da tirinha em uma página usando o BeautifulSoup.\n",
    "- Fazer download e salvar a imagem da tirinha no disco rígido. Dica de função a ser utilizada para salvar as tirinhas: iter_content().\n",
    "- Encontrar o URL do link Previous Comic (Tirinha anterior) e repetir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29740e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "url = 'http://xkcd.com/'\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "# para fazer o download de todas as tirinhas já publicadas basta substitir o loop por:\n",
    "# while not url.endswith('#')\n",
    "n = 0\n",
    "while n <= 49: \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    comic_src = soup.find('div', id='comic').img['src']\n",
    "    comic_url = f\"http:{comic_src}\"\n",
    "    image_response = requests.get(comic_url)\n",
    "    image_response.raise_for_status()\n",
    "    filename = comic_url.split('/')[-1]\n",
    "    with open('images/' + filename, 'wb') as file:\n",
    "        for content in image_response.iter_content(100000):\n",
    "            file.write(content)\n",
    "\n",
    "    previous = soup.find('a', rel='prev')\n",
    "    url = f\"http://xkcd.com{previous['href']}\"\n",
    "    n+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
